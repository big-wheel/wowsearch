// Jest Snapshot v1, https://goo.gl/fbAQLP

exports[`parseElementTree spec 1`] = `
DocumentNode {
  "children": Array [
    TextNode {
      "type": "text",
      "value": "robots.js — is parser for robots.txt files for node.js.",
    },
    LvlNode {
      "anchor": "user-content-installation",
      "children": Array [
        TextNode {
          "type": "text",
          "value": "It's recommended to install via npm:",
        },
        TextNode {
          "type": "text",
          "value": "$ npm install -g robots",
        },
      ],
      "level": 2,
      "type": "lvl",
      "value": "Installation",
    },
    LvlNode {
      "anchor": "user-content-usage",
      "children": Array [
        TextNode {
          "type": "text",
          "value": "Here's an example of using robots.js:",
        },
        TextNode {
          "type": "text",
          "value": "var robots = require('robots')  , parser = new robots.RobotsParser(); parser.setUrl('http://nodeguide.ru/robots.txt', function(parser, success) {  if(success) {    parser.canFetch('*', '/doc/dailyjs-nodepad/', function (access) {      if (access) {        // parse url      }    });  }});",
        },
        TextNode {
          "type": "text",
          "value": "Default crawler user-agent is:",
        },
        TextNode {
          "type": "text",
          "value": "Mozilla/5.0 (X11; Linux i686; rv:5.0) Gecko/20100101 Firefox/5.0
",
        },
        TextNode {
          "type": "text",
          "value": "Here's an example of using another user-agent and more detailed callback:",
        },
        TextNode {
          "type": "text",
          "value": "var robots = require('robots')  , parser = new robots.RobotsParser(                'http://nodeguide.ru/robots.txt',                'Mozilla/5.0 (compatible; RobotTxtBot/1.0)',                after_parse            );            function after_parse(parser, success) {  if(success) {    parser.canFetch('*', '/doc/dailyjs-nodepad/', function (access, url, reason) {      if (access) {        console.log(' url: '+url+', access: '+access);        // parse url ...      }    });  }};",
        },
        TextNode {
          "type": "text",
          "value": "Here's an example of getting list of sitemaps:",
        },
        TextNode {
          "type": "text",
          "value": "var robots = require('robots')  , parser = new robots.RobotsParser(); parser.setUrl('http://nodeguide.ru/robots.txt', function(parser, success) {  if(success) {    parser.getSitemaps(function(sitemaps) {      // sitemaps — array    });  }});",
        },
        TextNode {
          "type": "text",
          "value": "Here's an example of getCrawlDelay usage:",
        },
        TextNode {
          "type": "text",
          "value": "    var robots = require('robots')      , parser = new robots.RobotsParser();     // for example:    //    // $ curl -s http://nodeguide.ru/robots.txt    //    // User-agent: Google-bot    // Disallow: /     // Crawl-delay: 2    //    // User-agent: *    // Disallow: /    // Crawl-delay: 2     parser.setUrl('http://nodeguide.ru/robots.txt', function(parser, success) {      if(success) {        var GoogleBotDelay = parser.getCrawlDelay(\\"Google-bot\\");        // ...      }    });",
        },
        TextNode {
          "type": "text",
          "value": "An example of passing options to the HTTP request:",
        },
        TextNode {
          "type": "text",
          "value": "var options = {  headers:{    Authorization:\\"Basic \\" + new Buffer(\\"username:password\\").toString(\\"base64\\")}} var robots = require('robots')  , parser = new robots.RobotsParser(null, options); parser.setUrl('http://nodeguide.ru/robots.txt', function(parser, success) {  ...});",
        },
      ],
      "level": 2,
      "type": "lvl",
      "value": "Usage",
    },
    LvlNode {
      "anchor": "user-content-api",
      "children": Array [
        TextNode {
          "type": "text",
          "value": "RobotsParser — main class. This class provides a set of methods to read,
    parse and answer questions about a single robots.txt file.",
        },
        TextNode {
          "type": "text",
          "value": "setUrl(url, read) — sets the URL referring to a robots.txt file.
      by default, invokes read() method.
      If read is a function, it is called once the remote file is downloaded and parsed, and it
      takes in two arguments: the first is the parser itself, and the second is a boolean
      which is True if the the remote file was successfully parsed.",
        },
        TextNode {
          "type": "text",
          "value": "read(after_parse) — reads the robots.txt URL and feeds it to the parser",
        },
        TextNode {
          "type": "text",
          "value": "parse(lines) — parse the input lines from a robots.txt file",
        },
        TextNode {
          "type": "text",
          "value": "canFetch(userAgent, url, callback) — using the parsed robots.txt decide if
      userAgent can fetch url. Callback function:
      function callback(access, url, reason) { ... }
      where:
      
        access — can this url be fetched. true/false.
        url — target url
        reason — reason for access. Object:
          
            type — valid values: 'statusCode', 'entry', 'defaultEntry', 'noRule'
            entry — an instance of lib/Entry.js:. Only for types: 'entry', 'defaultEntry'
            statusCode — http response status code for url. Only for type 'statusCode'
          
        
      
    ",
        },
        TextNode {
          "type": "text",
          "value": "canFetchSync(userAgent, url) — using the parsed robots.txt decide if
      userAgent can fetch url. Return true/false.",
        },
        TextNode {
          "type": "text",
          "value": "getCrawlDelay(userAgent) — returns Crawl-delay for the certain userAgent",
        },
        TextNode {
          "type": "text",
          "value": "getSitemaps(sitemaps) — gets Sitemaps from parsed robots.txt",
        },
        TextNode {
          "type": "text",
          "value": "getDisallowedPaths(userAgent) — gets paths explictly disallowed for the user agent specified AND *",
        },
      ],
      "level": 2,
      "type": "lvl",
      "value": "API",
    },
    LvlNode {
      "anchor": "user-content-license",
      "children": Array [
        TextNode {
          "type": "text",
          "value": "See LICENSE
    file.",
        },
      ],
      "level": 2,
      "type": "lvl",
      "value": "License",
    },
    LvlNode {
      "anchor": "user-content-resources",
      "children": Array [
        TextNode {
          "type": "text",
          "value": "Robots.txt Specifications by Google",
        },
        TextNode {
          "type": "text",
          "value": "Robots.txt parser for python",
        },
        TextNode {
          "type": "text",
          "value": "A Standard for Robot Exclusion",
        },
      ],
      "level": 1,
      "type": "lvl",
      "value": "Resources",
    },
  ],
  "global": Map {
    "lvl0" => "robots.js",
  },
  "type": "document",
  "value": null,
}
`;

exports[`parseElementTree spec without global 1`] = `
DocumentNode {
  "children": Array [
    LvlNode {
      "anchor": "user-content-robotsjs",
      "children": Array [
        TextNode {
          "type": "text",
          "value": "robots.js — is parser for robots.txt files for node.js.",
        },
        LvlNode {
          "anchor": "user-content-installation",
          "children": Array [
            TextNode {
              "type": "text",
              "value": "It's recommended to install via npm:",
            },
            TextNode {
              "type": "text",
              "value": "$ npm install -g robots",
            },
          ],
          "level": 2,
          "type": "lvl",
          "value": "Installation",
        },
        LvlNode {
          "anchor": "user-content-usage",
          "children": Array [
            TextNode {
              "type": "text",
              "value": "Here's an example of using robots.js:",
            },
            TextNode {
              "type": "text",
              "value": "var robots = require('robots')  , parser = new robots.RobotsParser(); parser.setUrl('http://nodeguide.ru/robots.txt', function(parser, success) {  if(success) {    parser.canFetch('*', '/doc/dailyjs-nodepad/', function (access) {      if (access) {        // parse url      }    });  }});",
            },
            TextNode {
              "type": "text",
              "value": "Default crawler user-agent is:",
            },
            TextNode {
              "type": "text",
              "value": "Mozilla/5.0 (X11; Linux i686; rv:5.0) Gecko/20100101 Firefox/5.0
",
            },
            TextNode {
              "type": "text",
              "value": "Here's an example of using another user-agent and more detailed callback:",
            },
            TextNode {
              "type": "text",
              "value": "var robots = require('robots')  , parser = new robots.RobotsParser(                'http://nodeguide.ru/robots.txt',                'Mozilla/5.0 (compatible; RobotTxtBot/1.0)',                after_parse            );            function after_parse(parser, success) {  if(success) {    parser.canFetch('*', '/doc/dailyjs-nodepad/', function (access, url, reason) {      if (access) {        console.log(' url: '+url+', access: '+access);        // parse url ...      }    });  }};",
            },
            TextNode {
              "type": "text",
              "value": "Here's an example of getting list of sitemaps:",
            },
            TextNode {
              "type": "text",
              "value": "var robots = require('robots')  , parser = new robots.RobotsParser(); parser.setUrl('http://nodeguide.ru/robots.txt', function(parser, success) {  if(success) {    parser.getSitemaps(function(sitemaps) {      // sitemaps — array    });  }});",
            },
            TextNode {
              "type": "text",
              "value": "Here's an example of getCrawlDelay usage:",
            },
            TextNode {
              "type": "text",
              "value": "    var robots = require('robots')      , parser = new robots.RobotsParser();     // for example:    //    // $ curl -s http://nodeguide.ru/robots.txt    //    // User-agent: Google-bot    // Disallow: /     // Crawl-delay: 2    //    // User-agent: *    // Disallow: /    // Crawl-delay: 2     parser.setUrl('http://nodeguide.ru/robots.txt', function(parser, success) {      if(success) {        var GoogleBotDelay = parser.getCrawlDelay(\\"Google-bot\\");        // ...      }    });",
            },
            TextNode {
              "type": "text",
              "value": "An example of passing options to the HTTP request:",
            },
            TextNode {
              "type": "text",
              "value": "var options = {  headers:{    Authorization:\\"Basic \\" + new Buffer(\\"username:password\\").toString(\\"base64\\")}} var robots = require('robots')  , parser = new robots.RobotsParser(null, options); parser.setUrl('http://nodeguide.ru/robots.txt', function(parser, success) {  ...});",
            },
          ],
          "level": 2,
          "type": "lvl",
          "value": "Usage",
        },
        LvlNode {
          "anchor": "user-content-api",
          "children": Array [
            TextNode {
              "type": "text",
              "value": "RobotsParser — main class. This class provides a set of methods to read,
    parse and answer questions about a single robots.txt file.",
            },
            TextNode {
              "type": "text",
              "value": "setUrl(url, read) — sets the URL referring to a robots.txt file.
      by default, invokes read() method.
      If read is a function, it is called once the remote file is downloaded and parsed, and it
      takes in two arguments: the first is the parser itself, and the second is a boolean
      which is True if the the remote file was successfully parsed.",
            },
            TextNode {
              "type": "text",
              "value": "read(after_parse) — reads the robots.txt URL and feeds it to the parser",
            },
            TextNode {
              "type": "text",
              "value": "parse(lines) — parse the input lines from a robots.txt file",
            },
            TextNode {
              "type": "text",
              "value": "canFetch(userAgent, url, callback) — using the parsed robots.txt decide if
      userAgent can fetch url. Callback function:
      function callback(access, url, reason) { ... }
      where:
      
        access — can this url be fetched. true/false.
        url — target url
        reason — reason for access. Object:
          
            type — valid values: 'statusCode', 'entry', 'defaultEntry', 'noRule'
            entry — an instance of lib/Entry.js:. Only for types: 'entry', 'defaultEntry'
            statusCode — http response status code for url. Only for type 'statusCode'
          
        
      
    ",
            },
            TextNode {
              "type": "text",
              "value": "canFetchSync(userAgent, url) — using the parsed robots.txt decide if
      userAgent can fetch url. Return true/false.",
            },
            TextNode {
              "type": "text",
              "value": "getCrawlDelay(userAgent) — returns Crawl-delay for the certain userAgent",
            },
            TextNode {
              "type": "text",
              "value": "getSitemaps(sitemaps) — gets Sitemaps from parsed robots.txt",
            },
            TextNode {
              "type": "text",
              "value": "getDisallowedPaths(userAgent) — gets paths explictly disallowed for the user agent specified AND *",
            },
          ],
          "level": 2,
          "type": "lvl",
          "value": "API",
        },
        LvlNode {
          "anchor": "user-content-license",
          "children": Array [
            TextNode {
              "type": "text",
              "value": "See LICENSE
    file.",
            },
          ],
          "level": 2,
          "type": "lvl",
          "value": "License",
        },
        LvlNode {
          "anchor": "user-content-resources",
          "children": Array [
            TextNode {
              "type": "text",
              "value": "Robots.txt Specifications by Google",
            },
            TextNode {
              "type": "text",
              "value": "Robots.txt parser for python",
            },
            TextNode {
              "type": "text",
              "value": "A Standard for Robot Exclusion",
            },
          ],
          "level": 1,
          "type": "lvl",
          "value": "Resources",
        },
      ],
      "level": 0,
      "type": "lvl",
      "value": "robots.js",
    },
  ],
  "global": Map {},
  "type": "document",
  "value": null,
}
`;
