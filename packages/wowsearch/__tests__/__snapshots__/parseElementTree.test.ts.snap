// Jest Snapshot v1, https://goo.gl/fbAQLP

exports[`parseElementTree spec 1`] = `
DocumentNode {
  "children": Array [
    LvlNode {
      "anchor": "user-content-robotsjs",
      "children": Array [
        TextNode {
          "type": "text",
          "value": "robots.js — is parser for robots.txt files for node.js.",
        },
        LvlNode {
          "anchor": "user-content-usage",
          "children": Array [
            TextNode {
              "type": "text",
              "value": "Here's an example of using robots.js:",
            },
            TextNode {
              "type": "text",
              "value": "var robots = require('robots')  , parser = new robots.RobotsParser(); parser.setUrl('http://nodeguide.ru/robots.txt', function(parser, success) {  if(success) {    parser.canFetch('*', '/doc/dailyjs-nodepad/', function (access) {      if (access) {        // parse url      }    });  }});",
            },
            TextNode {
              "type": "text",
              "value": "Default crawler user-agent is:",
            },
            TextNode {
              "type": "text",
              "value": "Mozilla/5.0 (X11; Linux i686; rv:5.0) Gecko/20100101 Firefox/5.0
",
            },
            TextNode {
              "type": "text",
              "value": "Here's an example of using another user-agent and more detailed callback:",
            },
            TextNode {
              "type": "text",
              "value": "var robots = require('robots')  , parser = new robots.RobotsParser(                'http://nodeguide.ru/robots.txt',                'Mozilla/5.0 (compatible; RobotTxtBot/1.0)',                after_parse            );            function after_parse(parser, success) {  if(success) {    parser.canFetch('*', '/doc/dailyjs-nodepad/', function (access, url, reason) {      if (access) {        console.log(' url: '+url+', access: '+access);        // parse url ...      }    });  }};",
            },
            TextNode {
              "type": "text",
              "value": "Here's an example of getting list of sitemaps:",
            },
            TextNode {
              "type": "text",
              "value": "var robots = require('robots')  , parser = new robots.RobotsParser(); parser.setUrl('http://nodeguide.ru/robots.txt', function(parser, success) {  if(success) {    parser.getSitemaps(function(sitemaps) {      // sitemaps — array    });  }});",
            },
            TextNode {
              "type": "text",
              "value": "Here's an example of getCrawlDelay usage:",
            },
            TextNode {
              "type": "text",
              "value": "    var robots = require('robots')      , parser = new robots.RobotsParser();     // for example:    //    // $ curl -s http://nodeguide.ru/robots.txt    //    // User-agent: Google-bot    // Disallow: /     // Crawl-delay: 2    //    // User-agent: *    // Disallow: /    // Crawl-delay: 2     parser.setUrl('http://nodeguide.ru/robots.txt', function(parser, success) {      if(success) {        var GoogleBotDelay = parser.getCrawlDelay(\\"Google-bot\\");        // ...      }    });",
            },
            TextNode {
              "type": "text",
              "value": "An example of passing options to the HTTP request:",
            },
            TextNode {
              "type": "text",
              "value": "var options = {  headers:{    Authorization:\\"Basic \\" + new Buffer(\\"username:password\\").toString(\\"base64\\")}} var robots = require('robots')  , parser = new robots.RobotsParser(null, options); parser.setUrl('http://nodeguide.ru/robots.txt', function(parser, success) {  ...});",
            },
            TextNode {
              "type": "text",
              "value": "RobotsParser — main class. This class provides a set of methods to read,
    parse and answer questions about a single robots.txt file.",
            },
            TextNode {
              "type": "text",
              "value": "setUrl(url, read) — sets the URL referring to a robots.txt file.
      by default, invokes read() method.
      If read is a function, it is called once the remote file is downloaded and parsed, and it
      takes in two arguments: the first is the parser itself, and the second is a boolean
      which is True if the the remote file was successfully parsed.",
            },
            TextNode {
              "type": "text",
              "value": "See LICENSE
    file.",
            },
            TextNode {
              "type": "text",
              "value": "Robots.txt Specifications by Google",
            },
          ],
          "level": 2,
          "type": "lvl",
          "value": "Usage",
        },
        TextNode {
          "type": "text",
          "value": "It's recommended to install via npm:",
        },
        TextNode {
          "type": "text",
          "value": "$ npm install -g robots",
        },
        LvlNode {
          "anchor": "user-content-api",
          "children": Array [
            TextNode {
              "type": "text",
              "value": "read(after_parse) — reads the robots.txt URL and feeds it to the parser",
            },
            TextNode {
              "type": "text",
              "value": "Robots.txt parser for python",
            },
          ],
          "level": 2,
          "type": "lvl",
          "value": "API",
        },
        LvlNode {
          "anchor": "user-content-license",
          "children": Array [
            TextNode {
              "type": "text",
              "value": "A Standard for Robot Exclusion",
            },
          ],
          "level": 2,
          "type": "lvl",
          "value": "License",
        },
        TextNode {
          "type": "text",
          "value": "parse(lines) — parse the input lines from a robots.txt file",
        },
      ],
      "level": 0,
      "type": "lvl",
      "value": "robots.js",
    },
  ],
  "global": Map {},
  "type": "document",
  "value": null,
}
`;
